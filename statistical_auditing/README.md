In evaluating representational harm in diffusion models, however, one quickly runs into a thorny question: what are the inherent classifications (e.g. race/ethnicity) of an artificial generation? When we run algorithms on tabular datasets, we are satisfied with group/subgroup algorithmic fairness guarantees because the underlying subjects have self-identified into specific groups. Generated images, however, have no such underlying grouping, which raises fundamental epistemic questions about whether the "fairness problem" is underspecified. Indeed, how should we judge what category a generated subject belongs to? We might naturally begin with skin color, hair length, and facial features, although using these criteria alone has the potential to reify stereotypes. Examining such features also elides the rich detail encased in the context of every image, such as clothing or background details. Many examples of these alternative signals come from semiotics theory. In his essay "Romans in Films" in the collection *Mythologies*, for instance, semiotician Roland Barthes writes that in American cimena, one can easily identify Romans not by their skin color or face but instead by hairstyle: "Romans are Roman by the most legible of signs, that bit of toupee over the forehead." Clearly, there is much nuance in inferring someone's background from an image, artifically generated or not. 

In an attempt to begin answering this question, we propose in this paper a statistical auditing method to leverage \emph{the model's own internal representation} of the world to characterize its outputs. In particular, while it is difficult (in fact, impossible) to say what class a generated subject belongs to, it is possible to say what classes \emph{the model thinks} the subject belongs to. Our proposed method assumes every subject is some mixture over some classes, and solves for the mixture weights via a convex program. While we find that our proposed method ultimately is not performant during empirical experiments, we uncover--to the best of our knowledge--a novel empirical finding about the behavior of diffusion models while investigating why. 
